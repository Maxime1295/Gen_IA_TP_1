{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each practical exercise (TP), please work in groups of two or three. Then, create a **private GitHub repository** and add me (my GitHub is **arthur-75**) to your project. Finally, share the link to your project (or TP) under  [Practical Exercises](https://docs.google.com/spreadsheets/d/1V-YKgHn71FnwjoFltDhWsPJS7uIuAh9lj6SP2DSCvlY/edit?usp=sharing) and make sure to choose your **team name** :-)\n",
    "\n",
    "# **Variational Autoencoders on CelebA Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 128\n",
    "NUM_FEATURES = 128\n",
    "Z_DIM = 200\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 30\n",
    "BETA = 2000  # Weight on reconstruction loss\n",
    "LOAD_MODEL = False\n",
    "DEVICE = \"cuda\" \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1: Dataset Preparation and Exploration**\n",
    "\n",
    "1. **Obtain the CelebA dataset** from Kaggle and **unzip** it into a suitable directory (e.g., `img_align_celeba`).  \n",
    "   * [CelebFaces Attributes (CelebA) Dataset](https://www.kaggle.com/datasets/jessicali9530/celeba-dataset)   \n",
    "2. **Check the data structure**:  \n",
    "   * Typically, **ImageFolder** expects subfolders for each class, but here you only have one folder with all images. Adapt your **folder path** accordingly.  \n",
    "3. **Create a Transform** to:  \n",
    "   * **Resize** images to 32×32 (smaller dimension for quicker training).  \n",
    "   * **Convert** to tensors (and optionally normalize if you like).  \n",
    "4. **Wrap the dataset** in a **DataLoader** with:  \n",
    "\n",
    "   * `batch_size`, `shuffle=True`.  \n",
    "   * `drop_last=True` to ensure each batch is exactly the specified size (especially for large images).  \n",
    "5. **Visualize** a small batch to ensure everything is loading correctly (e.g., using a grid display)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to CelebA images\n",
    "DATA_PATH = xxx\n",
    "\n",
    "# Transforms: resize to 32x32 and normalize to [0,1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad((32, 32)),\n",
    "    transforms.ToTensor()  # values in [0,1]\n",
    "])\n",
    "\n",
    "# Dataset & DataLoader\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=os.path.dirname(DATA_PATH),  # Folder containing subfolder \"img_align_celeba\"\n",
    "    transform=transform\n",
    ")\n",
    "# NB: If you have a specific folder structure, adjust accordingly\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Utility: show a batch of faces\n",
    "def show_batch(images, title=\"\"):\n",
    "    grid_img = utils.make_grid(images, nrow=8)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(grid_img.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Test: Show a sample batch\n",
    "data_iter = iter(train_loader)\n",
    "images, _ = next(data_iter)\n",
    "show_batch(images, title=\"Training Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Why might you want to **normalize** to a different range (e.g., `[-1, 1]`) instead of `[0, 1]`?\n",
    "\n",
    "## **2: Model Architecture**\n",
    "\n",
    "### **2.1 Encoder**\n",
    "\n",
    "* The encoder uses **convolutional layers** to downsample the image from 32×32 until it’s a small spatial representation.  \n",
    "* Use **BatchNorm** and **LeakyReLU** activations to stabilize training and handle faces with various lighting conditions.  \n",
    "* Finally, flatten this representation into a **feature vector** and produce:  \n",
    "  * **μ**: The mean of the latent distribution.  \n",
    "  * **log(σ2)**: The log-variance of the latent distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim=Z_DIM):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(CHANNELS, NUM_FEATURES, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES), #Batch normalisation \n",
    "            nn.LeakyReLU(0.2, inplace=True), #activation functuon \n",
    "\n",
    "            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES), #Batch normalisation \n",
    "            nn.LeakyReLU(0.2, inplace=True),  #activation functuon \n",
    "\n",
    "            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES), #Batch normalisation  \n",
    "            nn.LeakyReLU(0.2, inplace=True),  #activation functuon \n",
    "\n",
    "            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Flatten + produce mu & logvar\n",
    "        # The output shape must be found dynamically\n",
    "        # Hardcode if you know the shape after 4 conv layers\n",
    "        # If IMAGE_SIZE=32 & 4 strided layers of stride=2 => output is 2x2\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 2x2 out => size is (NUM_FEATURES * 2 * 2) = 128 * 2 * 2\n",
    "        fc_in_features = NUM_FEATURES * (IMAGE_SIZE // 2**4) ** 2\n",
    "        self.mu = nn.Linear(fc_in_features, z_dim)\n",
    "        self.logvar = nn.Linear(fc_in_features, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.flatten(x)\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2.2 Sampling**\n",
    "\n",
    "* Implement the **reparameterization trick**  \n",
    "* This allows gradients to flow back through μ and σ, despite sampling being a stochastic process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    def forward(self, mu, logvar):\n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2.3 Decoder**\n",
    "\n",
    "* Invert the process:  \n",
    "  1. **Linear** layer maps from latent space RzdimRzdim​ to a flattened feature representation.  \n",
    "  2. **Reshape** into a small spatial feature map.  \n",
    "  3. Use **ConvTranspose2d** (also called deconvolution) layers to upsample back to 32×32 images.  \n",
    "  4. Apply a **Sigmoid** at the end to ensure output pixel values are in \\[0,1\\]\\[0,1\\].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim=Z_DIM):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        # Reverse of flatten\n",
    "        fc_out_features = NUM_FEATURES * (IMAGE_SIZE // 2**4) ** 2\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim, fc_out_features),\n",
    "            nn.xxx(fc_out_features),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            # Reshape => (N, NUM_FEATURES, 2, 2) if 4 strided layers used\n",
    "            # We'll reshape in forward()\n",
    "\n",
    "            nn.ConvTranspose2d(NUM_FEATURES, NUM_FEATURES, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(NUM_FEATURES, NUM_FEATURES, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(NUM_FEATURES, NUM_FEATURES, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(NUM_FEATURES, CHANNELS, 3, stride=2, padding=1, output_padding=1),\n",
    "            # Output: (CHANNELS, 32, 32)\n",
    "            nn.Sigmoid()  # since we want [0,1] output\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        # Reshape -> (N, NUM_FEATURES, 2, 2)\n",
    "        x = x.view(-1, NUM_FEATURES, (IMAGE_SIZE // 2**4), (IMAGE_SIZE // 2**4))\n",
    "        x = self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Why might we use **BatchNorm** and **LeakyReLU** in the decoder?  \n",
    "How does the output `ConvTranspose2d` layer ensure the final resolution is 32×32?\n",
    "\n",
    "### **2.4 All together** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim=Z_DIM):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.sampling(mu, logvar)\n",
    "        recon = self.decoder(z)\n",
    "        return mu, logvar, recon, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **3: VAE Loss Function**\n",
    "\n",
    "**3.1 Reconstruction Loss**\n",
    "\n",
    "* Either **BCE (binary cross entropy)** or **MSE (mean squared error)**.  \n",
    "* For faces with continuous pixel intensities, **MSE** often works well, but BCE can be used if values are in \\[0,1\\]\\[0,1\\].\n",
    "\n",
    "**3.2 KL Divergence**\n",
    "\n",
    "* Encourages the learned distribution q(z∣x)q(z∣x) to be close to a standard normal p(z)p(z).  \n",
    "* The standard formula is:  \n",
    "  \n",
    "\n",
    "* Often scaled by a factor if you want a ββ-VAE (which might help with disentanglement but can reduce reconstruction quality).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vae_loss_fn(x, recon, mu, logvar, beta=BETA):\n",
    "    \"\"\"\n",
    "    x, recon: shape (N, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    mu, logvar: shape (N, Z_DIM)\n",
    "    \"\"\"\n",
    "    # 1) Reconstruction Loss (MSE or BCE)\n",
    "    # - Following your TF code => MSELoss with a BETA factor\n",
    "    recon_loss = F.mse_loss(recon, x, reduction=\"sum\") \n",
    "\n",
    "    # 2) KL Divergence\n",
    "    # kl_loss ~ -0.5 * sum(1 + logvar - mu^2 - exp(logvar)) \n",
    "    kl =     \u000b    kl_loss = kl / x.size(0)\n",
    "\n",
    "    total_loss = recon_loss + kl_loss * (beta / x.size(0))\n",
    "\n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "if LOAD_MODEL and os.path.exists(\"./models/vae.pt\"):\n",
    "    print(\"Loading existing model...\")\n",
    "    model.load_state_dict(torch.load(\"./models/vae.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4: Training Loop**\n",
    "\n",
    "1. **Initialize** the VAE and **optimizer** (e.g., Adam with a suitable learning rate).  \n",
    "2. **Iterate over epochs**:  \n",
    "   1. Set the model to **train** mode.  \n",
    "   2. For each batch:  \n",
    "      * Send images to the GPU/CPU.  \n",
    "      * Forward pass: encode → reparameterize → decode → get reconstructed images.  \n",
    "      * Compute **VAE loss** \\= recon loss \\+ KL divergence (optionally scaled by β).  \n",
    "      * **Backpropagate** and **update weights**.  \n",
    "   3. Print or log the **average loss** per epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = xxx(xxx).to(DEVICE)\n",
    "optimizer = optim.xxx(model.parameters(), lr=xxx)\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss_val = 0.0\n",
    "    total_rec_val = 0.0\n",
    "    total_kl_val = 0.0\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(xxx):\n",
    "        x = x.to(DEVICE)\n",
    "        optimizer.xxx()\n",
    "\n",
    "        mu, logvar, recon, _ = model(x)\n",
    "        loss, rec_loss, kl_loss = vae_loss_fn(xxx, xxx, xxx, xxx, xxx)\n",
    "\n",
    "        loss.xxx()\n",
    "        optimizer.xxx()\n",
    "\n",
    "        total_loss_val += loss.item()\n",
    "        total_rec_val += rec_loss.item()\n",
    "        total_kl_val += kl_loss.item()\n",
    "\n",
    "    avg_loss = total_loss_val / len(train_loader)\n",
    "    avg_rec = total_rec_val / len(train_loader)\n",
    "    avg_kl = total_kl_val / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Loss: {avg_loss:.4f} | Recon: {avg_rec:.4f} | KL: {avg_kl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### You must at least run one epoch and then load this pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state dictionaries\u000bpath= \"path\"\u000bmodel.load_state_dict(torch.load(path, map_location=DEVICE))\u000bmodel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5: Generating and Saving Images**\n",
    "\n",
    "1. **Save** the model periodically or once training completes.  \n",
    "2. **Sample** random latent vectors z∼N(0,I)z∼N(0,I).  \n",
    "3. **Decode** them into face images.  \n",
    "4. Display or save the generated images to track progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"./models/vae_face.pt\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Take a small batch to reconstruct\n",
    "data_iter = iter(train_loader)\n",
    "images, _ = next(data_iter)\n",
    "images = images[:8].to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu, logvar, recon,_ = xxx(images)\n",
    "\n",
    "# Visualize original vs. reconstructions\n",
    "def show_reconstructions(orig, recon):\n",
    "    \"\"\"\n",
    "    Show original (top row) and reconstructions (bottom row).\n",
    "    \"\"\"\n",
    "    n = orig.size(0)\n",
    "    plt.figure(figsize=(2*n, 4))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        plt.subplot(2, n, i+1)\n",
    "        plt.imshow(orig[i].permute(1,2,0).cpu().numpy())\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    for i in range(n):\n",
    "        # Recon\n",
    "        plt.subplot(2, n, n+i+1)\n",
    "        plt.imshow(recon[i].permute(1,2,0).detach().cpu().numpy())\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_reconstructions(images, recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "grid_width, grid_height = 10, 3\n",
    "z_sample = torch.randn((grid_width*grid_height, Z_DIM)).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    recon = xxx.decoder(xxx).detach()\n",
    "\n",
    "# Plot the generated images\n",
    "def show_generated(recon, w=grid_width, h=grid_height):\n",
    "    recon = recon.cpu()\n",
    "    fig = plt.figure(figsize=(16, 5))\n",
    "    for i in range(w*h):\n",
    "        ax = fig.add_subplot(h, w, i+1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(recon[i].permute(1,2,0).numpy())\n",
    "    plt.show()\n",
    "\n",
    "show_generated(recon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **6: Next Steps and Experiments (optional)**\n",
    "\n",
    "* **Experiment** with:  \n",
    "  1. Larger or smaller **image sizes** (e.g., 64×64) for better detail.  \n",
    "  2. Different **latent dimensions** (Zdim=100 vs. Zdim=200).  \n",
    "  3. Various **activations** (e.g., ReLU vs. LeakyReLU).  \n",
    "  4. Adjusting **β** to see how it affects the **quality** of generated faces and the **structure** of the latent space.\n",
    "\n",
    "## **(Nothing to do just play with it)**\n",
    "\n",
    "## **Step 7: Building a Custom Dataset with Attributes**\n",
    "\n",
    "Create a **custom PyTorch Dataset** that:\n",
    "\n",
    "1. Reads attributes from a **CSV file** (e.g., `list_attr_celeba.csv`).  \n",
    "2. Loads the corresponding **image** for each entry.  \n",
    "3. Returns a **(image, label)** pair, where `label` indicates the presence or absence of an attribute (e.g., “Blond\\_Hair”).\n",
    "\n",
    "### **7.1 Read the CSV**\n",
    "\n",
    "* **Load the CSV** using a library like **pandas** (`pd.read_csv(...)`).  \n",
    "* Inspect the column names. Often, the CSV has an **\"image\\_id\"** column plus columns for each attribute (e.g., `Blond_Hair` is \\-1 or \\+1).\n",
    "\n",
    "### **7.2 Implement a `__getitem__` Method**\n",
    "\n",
    "1. **Index** into the DataFrame to get the row of interest.  \n",
    "2. Retrieve the **image filename** and construct the **full path** (using `os.path.join(...)`).  \n",
    "3. Load the image with **PIL** (`Image.open(...)`) and **convert** to `\"RGB\"`.  \n",
    "4. Apply **transforms** (resize, to-tensor, normalize).  \n",
    "5. Extract the **attribute** value (e.g., `row[\"Blond_Hair\"]`):  \n",
    "   * If it’s `-1`, you could convert it to `0`; if it’s `+1`, convert to `1`, or keep them as is if you want to differentiate positive vs. negative explicitly.\n",
    "\n",
    "### **7.3 Create a DataLoader**\n",
    "\n",
    "* Use the **custom Dataset** in a PyTorch `DataLoader` with parameters like `batch_size`, `shuffle`, etc.  \n",
    "* Test with a single batch to confirm the shape of `(images, labels)`:  \n",
    "  * `images` should be `(B, 3, 32, 32)` if color images are resized to 32×32.  \n",
    "  * `labels` should be `(B,)` if it’s just a single attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppose you have a CSV with attributes:\n",
    "# /app/data/celeba-dataset/list_attr_celeba.csv\n",
    "# Convert -1/1 => 0/1 to be more standard if needed.\n",
    "\n",
    "# Load attributes\n",
    "# Using pandas:\n",
    "attributes = pd.read_csv(\"img_align_celeba/list_attr_celeba.csv\")\n",
    "print(attributes.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LABEL = \"Blond_Hair\" or \"Eyeglasses\"  # example\n",
    "\n",
    "# We now want to create a data loader that yields (image, label)\n",
    "# Quick approach: we can build a custom dataset. This is a conceptual snippet.\n",
    "\n",
    "# Example minimal custom dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CelebADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_folder, attr_csv, label=LABEL, transform=None):\n",
    "        self.transform = transform\n",
    "        self.img_folder = img_folder\n",
    "        self.df = pd.read_csv(attr_csv)\n",
    "        self.label = label\n",
    "        # adjust if needed => if original images are named 000001.jpg, etc.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Typically, row[\"image_id\"] might hold the filename, e.g. 1.jpg or something\n",
    "        file_name = row[\"image_id\"]  # adapt to match your CSV\n",
    "        path = os.path.join(self.img_folder, file_name)\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label_val = row[self.label]\n",
    "        # Convert -1 => 0, +1 => 1 if needed\n",
    "        return image, label_val\n",
    "\n",
    "# Then you create a DataLoader:\n",
    "label_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "img_folder = \"img_align_celeba/img_align_celeba\"\n",
    "label_dataset = CelebADataset(img_folder, \"img_align_celeba/list_attr_celeba.csv\",\n",
    "                              label=LABEL,\n",
    "                              transform=label_transform)\n",
    "label_loader = DataLoader(label_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# For attribute vector extraction, you'd:\n",
    "# 1) Accumulate latent codes for positives and negatives\n",
    "# 2) Subtract negative mean from positive mean\n",
    "# 3) That difference is your attribute direction in latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **8: Extracting an Attribute Vector in Latent Space**\n",
    "\n",
    "Identify how the **attribute** (e.g. “Blond\\_Hair”) is **encoded** in the VAE’s **latent space** by:\n",
    "\n",
    "1. Gathering latent codes for **positive** samples (attribute \\= \\+1) and for **negative** samples (attribute \\= \\-1).  \n",
    "2. **Subtracting** the mean latent code of the negatives from the mean latent code of the positives to get a **direction vector**.\n",
    "\n",
    "### **8.1 Sample Latent Codes from the Dataset**\n",
    "\n",
    "1. **Iterate** over batches from the labeled DataLoader.  \n",
    "2. **Forward** each batch through the **VAE** to obtain the latent representation (whether that’s `z` directly or you compute `z = reparameterize(mu, logvar)`).  \n",
    "3. Split your batch into **positives** (`attribute=+1`) and **negatives** (`attribute=-1`).  \n",
    "4. Keep a **running sum** of the latent vectors for each group and a count of how many have been added.\n",
    "\n",
    "### **8.2 Compute Mean Vectors**\n",
    "\n",
    "* Once you have the sums and counts for both **positive** and **negative** latents, compute their **means**:\n",
    "\n",
    "![][image1]\n",
    "\n",
    "### **8.3 Extract the Attribute Direction**\n",
    "\n",
    "* **Attribute Vector** \\=   \n",
    "* **Normalize** it if you want a unit vector. For example: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* This vector represents the **direction** in latent space that corresponds to transitioning from **negative** to **positive examples** of the attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_vector_from_label(loader, vae, embedding_dim, label_name=\"Attribute\", max_samples=10000):\n",
    "    \"\"\"\n",
    "    Replicates the logic from 'get_vector_from_label' in Torch.\n",
    "    \n",
    "    loader: a PyTorch DataLoader yielding (images, labels) or (images, attribute).\n",
    "            We assume attribute is in {-1, +1}.\n",
    "    vae:    a VAE model with .encoder => returns (mu, logvar, z) or .forward => (mu, logvar, recon).\n",
    "    embedding_dim: dimension of the latent space (z-dim).\n",
    "    label_name: just for printing, e.g. \"Blond_Hair\".\n",
    "    max_samples: how many positive or negative samples to gather before we stop.\n",
    "    \n",
    "    Returns: A direction vector in latent space (np.ndarray, shape=(embedding_dim,)).\n",
    "    \"\"\"\n",
    "    device = next(vae.parameters()).device\n",
    "    vae.eval()\n",
    "\n",
    "    # Accumulators\n",
    "    current_sum_POS = np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    current_n_POS = 0\n",
    "    current_mean_POS = np.zeros(embedding_dim, dtype=\"float32\")\n",
    "\n",
    "    current_sum_NEG = np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    current_n_NEG = 0\n",
    "    current_mean_NEG = np.zeros(embedding_dim, dtype=\"float32\")\n",
    "\n",
    "    current_vector = np.zeros(embedding_dim, dtype=\"float32\")\n",
    "    current_dist = 0.0\n",
    "\n",
    "    print(f\"label: {label_name}\")\n",
    "    print(\"images : POS move : NEG move : distance : Δ distance\")\n",
    "\n",
    "    # We'll iterate over the loader, collecting latents\n",
    "    for images, attributes in loader:\n",
    "        images = images.to(device)\n",
    "        attributes = attributes.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            recon, mu, logvar, z = vae(images)  # Suppose vae(...) => (mu, logvar, recon) or (mu, logvar, z)\n",
    "            # If your VAE only returns (mu, logvar, recon), remove z => or do z = model.sampling(mu, logvar)\n",
    "            # We'll assume the third output is z. Adjust as needed.\n",
    "        \n",
    "        # Move from GPU to CPU and to numpy\n",
    "        z_np = z.detach().cpu().numpy()  # shape (N, embedding_dim)\n",
    "        attr_np = attributes.detach().cpu().numpy()  # shape (N, )\n",
    "\n",
    "        # Filter out positives/negatives\n",
    "        mask_pos = np.where(attr_np == 1)[0]\n",
    "        mask_neg = np.where(attr_np == -1)[0]\n",
    "\n",
    "        z_POS = z_np[mask_pos]\n",
    "        z_NEG = z_np[mask_neg]\n",
    "\n",
    "        # Update partial sums\n",
    "        if len(z_POS) > 0:\n",
    "            current_sum_POS += np.sum(z_POS, axis=0)\n",
    "            old_mean_POS = current_mean_POS.copy()\n",
    "            current_n_POS += len(z_POS)\n",
    "            current_mean_POS = current_sum_POS / current_n_POS\n",
    "            movement_POS = np.linalg.norm(current_mean_POS - old_mean_POS)\n",
    "        else:\n",
    "            movement_POS = 0.0\n",
    "\n",
    "        if len(z_NEG) > 0:\n",
    "            current_sum_NEG += np.sum(z_NEG, axis=0)\n",
    "            old_mean_NEG = current_mean_NEG.copy()\n",
    "            current_n_NEG += len(z_NEG)\n",
    "            current_mean_NEG = current_sum_NEG / current_n_NEG\n",
    "            movement_NEG = np.linalg.norm(current_mean_NEG - old_mean_NEG)\n",
    "        else:\n",
    "            movement_NEG = 0.0\n",
    "\n",
    "        # Current vector is difference\n",
    "        current_vector = current_mean_POS - current_mean_NEG\n",
    "        new_dist = np.linalg.norm(current_vector)\n",
    "        dist_change = new_dist - current_dist\n",
    "\n",
    "        print(f\"{current_n_POS}    : {movement_POS:.3f}    : {movement_NEG:.3f}    : {new_dist:.3f}    : {dist_change:.3f}\")\n",
    "\n",
    "        current_dist = new_dist\n",
    "\n",
    "        # Stopping conditions\n",
    "        # 1) Enough samples\n",
    "        if current_n_POS >= max_samples and current_n_NEG >= max_samples:\n",
    "            pass\n",
    "        # 2) Movement is small\n",
    "        if (movement_POS + movement_NEG) < 0.08 and new_dist > 1e-8:\n",
    "            current_vector /= new_dist\n",
    "            print(f\"Found the {label_name} vector\")\n",
    "            break\n",
    "\n",
    "    return current_vector  # np array\n",
    "feature_vec = get_vector_from_label(label_loader, model, embedding_dim=Z_DIM, label_name=LABEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **9: Using the Attribute Vector**\n",
    "\n",
    "Once you have an **attribute vector** (e.g., “Blond\\_Hair”, “Eyeglasses”), you can **modify** new or existing images in latent space:\n",
    "\n",
    "### **9.1 Adding the Vector to Images**\n",
    "\n",
    "1. Take a small batch of images.  \n",
    "2. **Encode** them to get latent codes z.  \n",
    "3. **Add** multiples of the attribute vector V attr​ to each code:   \n",
    "   Z′ \\= z \\+ α ⋅ V attr​   \n",
    "   for a range of αα values (like \\-4 to \\+4).  \n",
    "4. **Decode** each ′z′ to reconstruct an image with more or less of that attribute (e.g., hair color changing).\n",
    "\n",
    "## **![][image2]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_vector_to_images(loader, vae, feature_vec, factors=None):\n",
    "    \"\"\"\n",
    "    Replicates 'add_vector_to_images' in PyTorch.\n",
    "    \n",
    "    loader: yields (images, labels).\n",
    "    vae: a trained VAE with .encoder(...) => (mu, logvar, z) or .forward(...) => ...\n",
    "    feature_vec: an np.array of shape (embedding_dim,).\n",
    "    factors: list of multiples to add to the latent code. e.g. [-4, -3, -2, -1, 0, 1, 2, 3, 4].\n",
    "    \"\"\"\n",
    "    if factors is None:\n",
    "        factors = [-4, -3, -2, -1, 0, 1, 2, 3, 4]\n",
    "\n",
    "    device = next(vae.parameters()).device\n",
    "    vae.eval()\n",
    "\n",
    "    # Take one batch from loader\n",
    "    images, _ = next(iter(loader))  # or loader.__iter__() if repeated\n",
    "    images = images.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, mu, logvar, z = vae(images)\n",
    "\n",
    "    # We'll show 5 images\n",
    "    n_to_show = min(5, images.size(0))\n",
    "\n",
    "    fig = plt.figure(figsize=(len(factors) + 1, n_to_show * 2))\n",
    "    counter = 1\n",
    "\n",
    "    for i in range(n_to_show):\n",
    "        # Original image\n",
    "        orig_img = images[i].detach().cpu().permute(1,2,0).numpy()\n",
    "        ax = fig.add_subplot(n_to_show, len(factors) + 1, counter)\n",
    "        counter += 1\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(orig_img)\n",
    "        \n",
    "        # For each factor\n",
    "        base_z = z[i].detach().cpu().numpy()  # shape (embedding_dim,)\n",
    "        for f in factors:\n",
    "            changed_z = base_z + feature_vec * f\n",
    "            changed_z_torch = torch.from_numpy(changed_z).unsqueeze(0).float().to(device)  # (1, z_dim)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                recon = vae.decoder(changed_z_torch)  # shape (1, C, H, W)\n",
    "                recon_img = recon[0].detach().cpu().permute(1,2,0).numpy()\n",
    "\n",
    "            ax = fig.add_subplot(n_to_show, len(factors) + 1, counter)\n",
    "            counter += 1\n",
    "            ax.axis(\"off\")\n",
    "            ax.imshow(recon_img)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# 2) Add vector to images\n",
    "add_vector_to_images(label_loader, model, feature_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **9.2 Morph Faces Between Two Latent Codes**\n",
    "\n",
    "* Pick **two different images** A and B.  \n",
    "* Encode them into zA​ and zB​.  \n",
    "* **Interpolate** in latent space with:z(α)=(1−α) z.A  +  α z.B, α∈\\[0,1\\]  \n",
    "* Decode each intermediate code to see a **smooth morph** from face A to face B.\n",
    "\n",
    "![][image3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def morph_faces(loader, vae, steps=10):\n",
    "    \"\"\"\n",
    "    Replicates 'morph_faces' logic in PyTorch.\n",
    "    \n",
    "    loader: yields (images, labels).\n",
    "    vae: trained VAE.\n",
    "    steps: how many interpolation steps to show.\n",
    "    \"\"\"\n",
    "    device = next(vae.parameters()).device\n",
    "    vae.eval()\n",
    "\n",
    "    # Take one batch, pick first 2 images\n",
    "    images, _ = next(iter(loader))\n",
    "    images = images[:2].to(device)  # shape (2, C, H, W)\n",
    "    if images.size(0) < 2:\n",
    "        print(\"Not enough images in the batch to morph.\")\n",
    "        return\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _,mu, logvar, z = vae(images)  # shape (2, z_dim)\n",
    "    \n",
    "    zA = z[0].detach().cpu().numpy()\n",
    "    zB = z[1].detach().cpu().numpy()\n",
    "\n",
    "    factors = np.linspace(0, 1, steps)\n",
    "    plt.figure(figsize=(2*steps, 2))\n",
    "\n",
    "    # show image A on the far left\n",
    "    idx = 1\n",
    "    for alpha in factors:\n",
    "        inter_z = zA * (1 - alpha) + zB * alpha\n",
    "        inter_z_torch = torch.from_numpy(inter_z).unsqueeze(0).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            recon = vae.decoder(inter_z_torch)  # shape (1, C, H, W)\n",
    "        recon_img = recon[0].detach().cpu().permute(1,2,0).numpy()\n",
    "\n",
    "        ax = plt.subplot(1, steps, idx)\n",
    "        idx += 1\n",
    "        ax.imshow(recon_img)\n",
    "        ax.set_title(f\"{alpha:.2f}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# 3) Morph two faces\n",
    "morph_faces(label_loader, model, steps=10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
