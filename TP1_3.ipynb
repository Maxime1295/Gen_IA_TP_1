{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each practical exercise (TP), please work in groups of two or three. Then, create a **private GitHub repository** and add me (my GitHub is **arthur-75**) to your project. Finally, share the link to your project (or TP) under  [Practical Exercises](https://docs.google.com/spreadsheets/d/1V-YKgHn71FnwjoFltDhWsPJS7uIuAh9lj6SP2DSCvlY/edit?usp=sharing) and make sure to choose your **team name** :-)\n",
    "\n",
    "# **Variational Autoencoders on CelebA Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 128\n",
    "NUM_FEATURES = 128\n",
    "Z_DIM = 200\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 30\n",
    "BETA = 2000  # Weight on reconstruction loss\n",
    "LOAD_MODEL = False\n",
    "DEVICE = \"cuda\" \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1: Dataset Preparation and Exploration**\n",
    "\n",
    "1. **Obtain the CelebA dataset** from Kaggle and **unzip** it into a suitable directory (e.g., `img_align_celeba`).  \n",
    "   * [CelebFaces Attributes (CelebA) Dataset](https://www.kaggle.com/datasets/jessicali9530/celeba-dataset)   \n",
    "2. **Check the data structure**:  \n",
    "   * Typically, **ImageFolder** expects subfolders for each class, but here you only have one folder with all images. Adapt your **folder path** accordingly.  \n",
    "3. **Create a Transform** to:  \n",
    "   * **Resize** images to 32Ã—32 (smaller dimension for quicker training).  \n",
    "   * **Convert** to tensors (and optionally normalize if you like).  \n",
    "4. **Wrap the dataset** in a **DataLoader** with:  \n",
    "   * `batch_size`, `shuffle=True`.  \n",
    "   * `drop_last=True` to ensure each batch is exactly the specified size (especially for large images).  \n",
    "5. **Visualize** a small batch to ensure everything is loading correctly (e.g., using a grid display)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to CelebA images\n",
    "DATA_PATH = xxx\n",
    "\n",
    "# Transforms: resize to 32x32 and normalize to [0,1]\n",
    "transform = transforms.Compose([\n",
    "    xxx.xxx((xxx, xxx)),\n",
    "    xxx.xxx()  # values in [0,1]\n",
    "])\n",
    "\n",
    "# Dataset & DataLoader\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=os.path.dirname(DATA_PATH),  # Folder containing subfolder \"img_align_celeba\"\n",
    "    transform=transform\n",
    ")\n",
    "# NB: If you have a specific folder structure, adjust accordingly\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    xxx,\n",
    "    xxx=xxx,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Utility: show a batch of faces\n",
    "def show_batch(images, title=\"\"):\n",
    "    grid_img = utils.make_grid(images, nrow=8)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(grid_img.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Test: Show a sample batch\n",
    "data_iter = iter(train_loader)\n",
    "images, _ = next(data_iter)\n",
    "show_batch(images, title=\"Training Samples\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
