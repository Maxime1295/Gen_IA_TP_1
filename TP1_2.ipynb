{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each practical exercise (TP), please work in groups of two or three. Then, create a **private GitHub repository** and add me (my GitHub is  **arthur-75**) to your project. Finally, share the link to your project (or TP) under  [Practical Exercises](https://docs.google.com/spreadsheets/d/1V-YKgHn71FnwjoFltDhWsPJS7uIuAh9lj6SP2DSCvlY/edit?usp=sharing) and make sure to choose your **team name** :-)\n",
    "\n",
    "# **Variational Autoencoders on Fashion MNIST**\n",
    "\n",
    "## **1: Data Preparation and Visualization**\n",
    "\n",
    "**Identical to the First Practical Exercise**\n",
    "\n",
    "**Goal**:\n",
    "\n",
    "* Load the **Fashion MNIST** dataset and **pad** images to 32×32.  \n",
    "* Create **DataLoaders** for training and validation.  \n",
    "* Visualize a few samples to confirm data integrity.\n",
    "\n",
    "**Key Points to Recall**:\n",
    "\n",
    "* **Why** do we pad the images from 28×28 to 32×32?  \n",
    "* **Which** transformations can help (e.g., normalization)?  \n",
    "* **How** do we shuffle and batch the data for efficient training?\n",
    "\n",
    "(Refer back to **Step 1** of the first exercise for detailed guidance.)\n",
    "\n",
    "## **2: Define the VAE Model**\n",
    "\n",
    "**Key Difference from a Standard Autoencoder**:\n",
    "\n",
    "* Instead of directly learning a **latent vector** zz, the VAE learns a **distribution** N(μ,σ2)  by predicting **μ**(mean) and **log⁡(σ2)** (log-variance).  \n",
    "* Use a **reparameterization trick**:\n",
    "\n",
    "**Guided Outline**:\n",
    "\n",
    "1. **Encoder**:  \n",
    "   * Convolutional layers to reduce spatial dimensions and extract features.  \n",
    "   * Two separate heads: one for **μ** (mean) and one for **log⁡(σ2)**(log-variance).  \n",
    "2. **Reparameterization** (in a function like `reparameterize(mu, logvar)`).  \n",
    "3. **Decoder**:  \n",
    "   * Transposed convolution layers to reconstruct the image from zz.  \n",
    "   * Output uses **Sigmoid** to ensure pixel values remain between 0 and 1\\.\n",
    "\n",
    "Why do we need separate heads for **μ** and **log⁡(σ2))**?  \n",
    "How does the reparameterization trick help with backpropagation through stochastic nodes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 32, 32), latent_dim=2):\n",
    "        \"\"\"\n",
    "        Variational Autoencoder initialization.\n",
    "        \n",
    "        Args:\n",
    "            input_shape (tuple): Shape of the input image. Default is (1, 32, 32).\n",
    "            latent_dim (int): Dimension of the latent representation.\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # ------------------\n",
    "        #     1. Encoder\n",
    "        # ------------------\n",
    "        # This block progressively reduces the spatial dimension of the input.\n",
    "        # Each Conv2D layer uses stride=2 to downsample.\n",
    "        self.encoder = nn.Sequential(\n",
    "            # First convolutional layer: Convert 1-channel input into 32 feature maps.\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(), # acivation function \n",
    "\n",
    "            # Second convolutional layer: Reduce spatial dimensions further.\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(), #acivation function \n",
    "\n",
    "            # Third convolutional layer: Extract higher-level features.\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),  # Add another Conv2D layer with 128 filters, kernel size 3, stride 2, padding 1\n",
    "            nn.ReLU(), #acivation function \n",
    "        )\n",
    "\n",
    "        # Dynamically calculate the flattened size after all convolution layers\n",
    "        flattened_size, decode_shape = self.calculate_flattened_size(self.encoder, input_shape)\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        #     2. Fully Connected Layers for Latent Space\n",
    "        # ------------------------------------------------\n",
    "\n",
    "        # Two separate heads for the VAE:\n",
    "        #  - fc_mu: Predicts the mean of the latent distribution\n",
    "        #  - fc_logvar: Predicts the log-variance of the latent distribution\n",
    "        self.fc_mu = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(flattened_size, latent_dim)  # Latent mean\n",
    "        )\n",
    "        self.fc_logvar = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(flattened_size, latent_dim)  # Latent log variance (Hint: same dimension as mu)\n",
    "        )\n",
    "\n",
    "        # ------------------\n",
    "        #     3. Decoder\n",
    "        # ------------------\n",
    "        # Inverts the encoder process with ConvTranspose2d (a.k.a. deconvolutions).\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, flattened_size),  # Map latent space back to the feature map\n",
    "            nn.Unflatten(decode_shape[0],decode_shape[1:]),  # Reshape to match the encoded feature map\n",
    "\n",
    "            # Transposed Convolution layers (Decoder)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1), # Add a conv transpose 2d\n",
    "            nn.ReLU(), # acivation function \n",
    "\n",
    "\n",
    "            # Next deconvolution layer\n",
    "            nn.ConvTranspose2d(64,32,kernel_size=3,stride=2, padding=1, output_padding=1),  # Add a ConvTranspose2d layer reducing from 64 channels to 32 channels and  kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "\n",
    "\n",
    "            nn.ReLU(), # acivation function \n",
    "\n",
    "            # Final deconvolution layer: Convert back to single-channel grayscale image\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # acivation function Output values should be between 0 and 1\n",
    "        \n",
    "        )\n",
    "\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick:\n",
    "        \n",
    "        z = mu + sigma * eps, where eps ~ N(0, I)\n",
    "\n",
    "        Args:\n",
    "            mu (Tensor): Mean of the latent distribution.\n",
    "            logvar (Tensor): Log-variance of the latent distribution.\n",
    "\n",
    "        Returns:\n",
    "            z (Tensor): Latent variable sampled from N(mu, sigma^2).\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)  # Convert log variance to standard deviation\n",
    "        eps = torch.randn_like(std)    # Sample noise from a normal distribution\n",
    "        return mu + std * eps\n",
    "\n",
    "    def calculate_flattened_size(self, model, input_shape):\n",
    "       #same as the last one \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            output = model(dummy_input)\n",
    "            return output.numel(), output.shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the VAE:\n",
    "        \n",
    "        1) Encode input into latent distribution parameters (mu, logvar).\n",
    "        2) Sample z using the reparameterization trick.\n",
    "        3) Decode z back to a reconstructed image.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input images.\n",
    "\n",
    "        Returns:\n",
    "            recon_x (Tensor): Reconstructed images.\n",
    "            mu (Tensor): Mean of latent distribution.\n",
    "            logvar (Tensor): Log-variance of latent distribution.\n",
    "            z (Tensor): Sampled latent variable.\n",
    "        \"\"\"\n",
    "        # Encode input\n",
    "        x_encoded = self.encoder(x)\n",
    "\n",
    "        # Compute mu and logvar\n",
    "        mu = self.fc_mu(x_encoded)\n",
    "        logvar = self.fc_logvar(x_encoded)\n",
    "\n",
    "        # Sample from the latent distribution\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decode latent vector to reconstruct the input\n",
    "        recon_x = self.decoder(z)\n",
    "        \n",
    "        return recon_x, mu, logvar, z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dimension consistency\n",
    "x_sample = torch.randn(1, 1, 32, 32)  # Example input\n",
    "model_test = Autoencoder()\n",
    "image_test, mu, logvar, z = model_test(x_sample)\n",
    "assert image_test.shape == x_sample.shape, \"Output dimensions do not match input dimensions!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3: VAE Loss Function and Training**\n",
    "\n",
    "1. **VAE Loss** combines:  \n",
    "   * **Reconstruction Loss** (e.g., Binary Cross Entropy or MSE)  \n",
    "   * **Kullback–Leibler (KL) Divergence** to regularize the latent space.  \n",
    "       \n",
    "2. **β-VAE**:  \n",
    "   * Introduces a hyperparameter β to **weight the KL term**.  \n",
    "   * If β=1, it’s the standard VAE.  \n",
    "   * Larger β puts more emphasis on the KL divergence, often leading to more **disentangled** representations but potentially poorer reconstructions.\n",
    "\n",
    "### **Training Flow:**\n",
    "\n",
    "* For each batch:  \n",
    "  1. Forward pass: get **μ, log⁡(σ2), z,** and **reconstruction**.  \n",
    "  2. Compute \\**vae\\_loss \\= reconstruction\\_loss \\+ β.KL\\_loss*.  \n",
    "  3. Backpropagate and **update parameters**.\n",
    "\n",
    "How does a larger β value change the **trade-off** between **reconstruction quality** and **latent disentanglement**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# 1. Define VAE Loss\n",
    "#######################\n",
    "def vae_loss(recon_x, x, mu, logvar, beta=1):\n",
    "    \"\"\"\n",
    "    Compute the Variational Autoencoder (VAE) loss function.\n",
    "    A combination of:\n",
    "      - Reconstruction loss (using BCE or MSE)\n",
    "      - KL divergence regularizer\n",
    "\n",
    "    Args:\n",
    "      recon_x (Tensor): Reconstructed images from the decoder.\n",
    "      x (Tensor): Original input images.\n",
    "      mu (Tensor): Mean of the latent distribution.\n",
    "      logvar (Tensor): Log-variance of the latent distribution.\n",
    "      beta (float): Weight for the KL term (β-VAE concept).\n",
    "\n",
    "    Returns:\n",
    "      total_loss (Tensor): Sum of reconstruction loss and β * KL divergence.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Reconstruction Loss\n",
    "    #    Measures how closely recon_x matches x. \n",
    "    #    Typically use Binary Cross Entropy (BCE) if inputs are normalized [0,1].\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "\n",
    "    # 2) KL Divergence\n",
    "    #    Encourages the approximate posterior (q(z|x)) to be close to a \n",
    "    #    standard normal prior p(z) ~ N(0,I).\n",
    "    kl_div = xxx  # \n",
    "    # Return the total VAE loss\n",
    "    return xxx + beta * xxx\n",
    "\n",
    "#######################\n",
    "# 2. Training Function\n",
    "#######################\n",
    "def train_model(model, train_loader, val_loader, epochs, beta=1):\n",
    "    \"\"\"\n",
    "    Trains a VAE model with a given β for the KL term.\n",
    "\n",
    "    Args:\n",
    "      model (nn.Module): VAE instance (encoder + decoder).\n",
    "      train_loader (DataLoader): Dataloader for training set.\n",
    "      val_loader (DataLoader): Dataloader for validation set.\n",
    "      epochs (int): Number of training epochs.\n",
    "      beta (float): Weight for the KL divergence in the VAE loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Choose an optimizer, e.g., Adam\n",
    "    optimizer = xxx # e.g., 1e-4\n",
    "    # Lists to store loss values for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to train mode\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Use tqdm to create a progress bar\n",
    "        tqdm_loader = tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        for x, _ in tqdm_loader:\n",
    "            # 1) Zero out gradients from previous iteration\n",
    "            xxx\n",
    "\n",
    "            # 2) Move the batch to the correct device (CPU, CUDA, or MPS)\n",
    "            x = x.to(xxx)  # Fill in your device, e.g., device, \"cuda\", or \"mps\"\n",
    "\n",
    "            # 3) Forward pass: encode -> reparameterize -> decode\n",
    "            recon_x, mu, logvar, _ = model(x)\n",
    "\n",
    "            # 4) Compute VAE loss\n",
    "            loss = vae_loss(recon_x, x, mu, logvar, beta=beta)\n",
    "\n",
    "            # 5) Backpropagation\n",
    "            xxx\n",
    "            # 5) update grad\n",
    "            xxx\n",
    "\n",
    "            # Accumulate total loss for this batch\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update tqdm progress bar with the current batch loss\n",
    "            tqdm_loader.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Average loss over all training samples\n",
    "        avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluate on validation data\n",
    "        avg_val_loss, _ = evaluate_model(model, val_loader)\n",
    "\n",
    "        # Print epoch statistics\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{epochs}], \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}\"\n",
    "        )\n",
    "\t train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "   plot_training_loss(train_losses, val_losses)\n",
    "\n",
    "\n",
    "#######################\n",
    "# 3. Evaluation Function\n",
    "#######################\n",
    "def evaluate_model(model, loader ,beta=xxx):\n",
    "    \"\"\"\n",
    "    Evaluates the VAE on a validation or test dataset.\n",
    "\n",
    "    Args:\n",
    "      model (nn.Module): VAE instance (encoder + decoder).\n",
    "      loader (DataLoader): Dataloader for validation/test set.\n",
    "\n",
    "    Returns:\n",
    "      avg_loss (float): Average loss across all validation samples.\n",
    "      ce_loss_placeholder (float): Placeholder if you want to track\n",
    "                                   additional metrics or losses.\n",
    "beta (float): Weight for the KL divergence in the VAE loss.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    ce_loss_placeholder = 0  # Example placeholder for separate metrics\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in tqdm(loader, desc=\"Evaluating\"):\n",
    "            x = x.to(xxx)  # Move data to the same device as the model\n",
    "\n",
    "            # Forward pass to get reconstruction and latent variables\n",
    "            recon_x, mu, logvar, _ = model(x)\n",
    "\n",
    "            # Compute VAE loss (without adjusting gradients)\n",
    "            loss = vae_loss(recon_x, x, mu, logvar, beta=xxx)  # e.g., same β as training\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            # Optionally compute or track other metrics here, e.g.:\n",
    "            # ce_loss_placeholder += some_other_metric(...)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss, ce_loss_placeholder\n",
    "\n",
    "def plot_training_loss(train_losses, val_losses):\n",
    "    \"\"\"\n",
    "    Plot the training and validation loss across epochs.\n",
    "\n",
    "    - train_losses: List of training losses per epoch.\n",
    "    - val_losses: List of validation losses per epoch.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\", marker=\"s\")\n",
    "    \n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "# Train and visualize Autoencoder\n",
    "vae = VAE().to(device)\n",
    "print(\"Training Autoencoder...\")\n",
    "train_model(vae, train_loader, val_loader, epochs=10,beta =1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You must at least run one epoch and then load this pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state dictionaries\u000bpath= \"path\"\u000bmodel.load_state_dict(torch.load(path, map_location=DEVICE))\u000bmodel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4: Reconstruct Images**\n",
    "\n",
    "**Similar to Step 4 in the First Practical Exercise**\n",
    "\n",
    "**Goal**:\n",
    "\n",
    "* Take a batch of **original images**.  \n",
    "* Pass them through the VAE to get **reconstructed images**.  \n",
    "* Plot and compare **side-by-side**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructions(model, loader_or_z, ex=True, num_samples=5):\n",
    "    \"\"\"\n",
    "    Plot original and reconstructed images from an Autoencoder (VAE).\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained VAE model.\n",
    "    - loader_or_z: Either a DataLoader (for real images) or sampled latent vectors.\n",
    "    - ex: If True, extracts a batch from DataLoader; otherwise, uses provided latent vectors.\n",
    "    - num_samples: Number of images to display.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    model = model.to(\"cpu\")  # Move to CPU for visualization\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if ex:  # Extract batch from DataLoader\n",
    "            for x, _ in loader_or_z:\n",
    "                recon_x, _, _, _ = model(x)\n",
    "                break\n",
    "        else:  # Use provided latent vectors\n",
    "            x = None  # No original images in this case\n",
    "            recon_x = model.decoder(loader_or_z)\n",
    "\n",
    "    # Convert tensors to NumPy for visualization\n",
    "    if x is not None:\n",
    "        x = x.cpu().numpy()\n",
    "    recon_x = recon_x.cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(num_samples):\n",
    "        # Plot Original images (if real data was used)\n",
    "        if x is not None:\n",
    "            plt.subplot(2, num_samples, i + 1)\n",
    "            plt.imshow(x[i].squeeze(), cmap=\"gray\")\n",
    "            plt.title(\"Original\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        # Plot Reconstructed images\n",
    "        plt.subplot(2, num_samples, i + 1 + num_samples)\n",
    "        plt.imshow(recon_x[i].squeeze(), cmap=\"gray\")\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_reconstructions( vae, train_loader, ex=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5: Visualize the Latent Space**\n",
    "\n",
    "**Similar to Step 5 in the First Practical Exercise**\n",
    "\n",
    "**Goal**:\n",
    "\n",
    "* Extract **2D latent embeddings** by forwarding images through the encoder.  \n",
    "* Plot them in a 2D scatter plot, color-coded by labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "vae = vae.to(device)  # Replace xxx with the correct device \n",
    "with torch.no_grad():\n",
    "    for x, y in train_loader:  # Iterate over training batches\n",
    "        xxx\n",
    "\n",
    "        xxx \n",
    "\n",
    "        # Convert to NumPy and store results\n",
    "        embeddings.append(latent.cpu().numpy())\n",
    "        labels.append(y.numpy())\n",
    "\n",
    "embeddings = np.concatenate(embeddings, axis=0)  \n",
    "labels = np.concatenate(labels, axis=0)  \n",
    "\n",
    "# Identify unique class labels\n",
    "unique_labels = np.unique(labels)\n",
    "vae=vae.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D embeddings\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "legend_handles = []\n",
    "for i, label in enumerate(unique_labels):\n",
    "    indices = np.where(labels == label)[0]\n",
    "    scatter = plt.scatter(\n",
    "        xxx[indices, 0], xxx[indices, 1],\n",
    "        c=[colors[i]], alpha=0.4, s=3, label=class_labels[label]\n",
    "    )\n",
    "    legend_handles.append(\n",
    "        Line2D([0], [0], marker='o', color='w', label=f'{class_labels[label]}',\n",
    "               markersize=10, markerfacecolor=colors[i])\n",
    "    )\n",
    "\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('2D Latent Space Visualization')\n",
    "plt.legend(handles=legend_handles, title='Classes', loc='upper left', fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **6: Generate New Images**\n",
    "\n",
    "**Similar to Step 6 in the First Practical Exercise**\n",
    "\n",
    "**Goal**:\n",
    "\n",
    "* Sample random points zz from a **Gaussian distribution** (e.g., N(0,I)) or from the learned ranges of μ and log⁡(σ2)).  \n",
    "* Pass these points through the **decoder**.  \n",
    "* Observe the generated images."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
