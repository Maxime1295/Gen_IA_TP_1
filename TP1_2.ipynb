{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each practical exercise (TP), please work in groups of two or three. Then, create a **private GitHub repository** and add me (my GitHub is  **arthur-75**) to your project. Finally, share the link to your project (or TP) under  [Practical Exercises](https://docs.google.com/spreadsheets/d/1V-YKgHn71FnwjoFltDhWsPJS7uIuAh9lj6SP2DSCvlY/edit?usp=sharing) and make sure to choose your **team name** :-)\n",
    "\n",
    "# **Variational Autoencoders on Fashion MNIST**\n",
    "\n",
    "## **1: Data Preparation and Visualization**\n",
    "\n",
    "**Identical to the First Practical Exercise**\n",
    "\n",
    "**Goal**:\n",
    "\n",
    "* Load the **Fashion MNIST** dataset and **pad** images to 32×32.  \n",
    "* Create **DataLoaders** for training and validation.  \n",
    "* Visualize a few samples to confirm data integrity.\n",
    "\n",
    "**Key Points to Recall**:\n",
    "\n",
    "* **Why** do we pad the images from 28×28 to 32×32?  \n",
    "* **Which** transformations can help (e.g., normalization)?  \n",
    "* **How** do we shuffle and batch the data for efficient training?\n",
    "\n",
    "(Refer back to **Step 1** of the first exercise for detailed guidance.)\n",
    "\n",
    "## **2: Define the VAE Model**\n",
    "\n",
    "**Key Difference from a Standard Autoencoder**:\n",
    "\n",
    "* Instead of directly learning a **latent vector** zz, the VAE learns a **distribution** N(μ,σ2)  by predicting **μ**(mean) and **log⁡(σ2)** (log-variance).  \n",
    "* Use a **reparameterization trick**:\n",
    "\n",
    "**Guided Outline**:\n",
    "\n",
    "1. **Encoder**:  \n",
    "   * Convolutional layers to reduce spatial dimensions and extract features.  \n",
    "   * Two separate heads: one for **μ** (mean) and one for **log⁡(σ2)**(log-variance).  \n",
    "2. **Reparameterization** (in a function like `reparameterize(mu, logvar)`).  \n",
    "3. **Decoder**:  \n",
    "   * Transposed convolution layers to reconstruct the image from zz.  \n",
    "   * Output uses **Sigmoid** to ensure pixel values remain between 0 and 1\\.\n",
    "\n",
    "Why do we need separate heads for **μ** and **log⁡(σ2))**?  \n",
    "How does the reparameterization trick help with backpropagation through stochastic nodes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each practical exercise (TP), please work in groups of two or three. Then, create a private GitHub repository and add me (my GitHub is  arthur-75) to your project. Finally, share the link to your project (or TP) under  Practical Exercises and make sure to choose your team name :-)\n",
    "Variational Autoencoders on Fashion MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Data Preparation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identical to the First Practical Exercise  \n",
    "Goal:  \n",
    "Load the Fashion MNIST dataset and pad images to 32×32.  \n",
    "Create DataLoaders for training and validation.  \n",
    "Visualize a few samples to confirm data integrity.  \n",
    "Key Points to Recall:  \n",
    "Why do we pad the images from 28×28 to 32×32?  \n",
    "Which transformations can help (e.g., normalization)?  \n",
    "How do we shuffle and batch the data for efficient training?  \n",
    "(Refer back to Step 1 of the first exercise for detailed guidance.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we pad the images from 28×28 to 32×32?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which transformations can help (e.g., normalization)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we shuffle and batch the data for efficient training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Define the VAE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Difference from a Standard Autoencoder:  \n",
    "Instead of directly learning a latent vector zz, the VAE learns a distribution N(μ,σ2)  by predicting μ(mean) and log⁡(σ2) (log-variance).  \n",
    "Use a reparameterization trick:  \n",
    "$$z=\\mu+\\sigma\\times\\epsilon,\\quad\\epsilon\\sim\\mathcal{N}(0,1)$$\n",
    "Guided Outline:\n",
    "Encoder:  \n",
    "Convolutional layers to reduce spatial dimensions and extract features.  \n",
    "Two separate heads: one for $$μ$$ (mean) and one for $$log⁡(σ2)$$(log-variance).  \n",
    "Reparameterization (in a function like reparameterize(mu, logvar)).  \n",
    "Decoder:  \n",
    "Transposed convolution layers to reconstruct the image from zz.\n",
    "Output uses Sigmoid to ensure pixel values remain between 0 and 1.\n",
    "Why do we need separate heads for μ and log⁡(σ2))?\n",
    "How does the reparameterization trick help with backpropagation through stochastic nodes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need separate heads for μ and log⁡(σ2))?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the reparameterization trick help with backpropagation through stochastic nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 32, 32), latent_dim=2):\n",
    "        \"\"\"\n",
    "        Variational Autoencoder initialization.\n",
    "        \n",
    "        Args:\n",
    "            input_shape (tuple): Shape of the input image. Default is (1, 32, 32).\n",
    "            latent_dim (int): Dimension of the latent representation.\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # ------------------\n",
    "        #     1. Encoder\n",
    "        # ------------------\n",
    "        # This block progressively reduces the spatial dimension of the input.\n",
    "        # Each Conv2D layer uses stride=2 to downsample.\n",
    "        self.encoder = nn.Sequential(\n",
    "            # First convolutional layer: Convert 1-channel input into 32 feature maps.\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(), # acivation function \n",
    "\n",
    "            # Second convolutional layer: Reduce spatial dimensions further.\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(), #acivation function \n",
    "\n",
    "            # Third convolutional layer: Extract higher-level features.\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),  # Add another Conv2D layer with 128 filters, kernel size 3, stride 2, padding 1\n",
    "            nn.ReLU(), #acivation function \n",
    "        )\n",
    "\n",
    "        # Dynamically calculate the flattened size after all convolution layers\n",
    "        flattened_size, decode_shape = self.calculate_flattened_size(self.encoder, input_shape)\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        #     2. Fully Connected Layers for Latent Space\n",
    "        # ------------------------------------------------\n",
    "\n",
    "        # Two separate heads for the VAE:\n",
    "        #  - fc_mu: Predicts the mean of the latent distribution\n",
    "        #  - fc_logvar: Predicts the log-variance of the latent distribution\n",
    "        self.fc_mu = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(flattened_size, latent_dim)  # Latent mean\n",
    "        )\n",
    "        self.fc_logvar = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(flattened_size, latent_dim)  # Latent log variance (Hint: same dimension as mu)\n",
    "        )\n",
    "\n",
    "        # ------------------\n",
    "        #     3. Decoder\n",
    "        # ------------------\n",
    "        # Inverts the encoder process with ConvTranspose2d (a.k.a. deconvolutions).\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, flattened_size),  # Map latent space back to the feature map\n",
    "            nn.Unflatten(decode_shape[0],decode_shape[1:]),  # Reshape to match the encoded feature map\n",
    "\n",
    "            # Transposed Convolution layers (Decoder)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1), # Add a conv transpose 2d\n",
    "            nn.ReLU(), # acivation function \n",
    "\n",
    "\n",
    "            # Next deconvolution layer\n",
    "            nn.ConvTranspose2d(64,32,kernel_size=3,stride=2, padding=1, output_padding=1),  # Add a ConvTranspose2d layer reducing from 64 channels to 32 channels and  kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "\n",
    "\n",
    "            nn.ReLU(), # acivation function \n",
    "\n",
    "            # Final deconvolution layer: Convert back to single-channel grayscale image\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # acivation function Output values should be between 0 and 1\n",
    "        \n",
    "        )\n",
    "\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick:\n",
    "        \n",
    "        z = mu + sigma * eps, where eps ~ N(0, I)\n",
    "\n",
    "        Args:\n",
    "            mu (Tensor): Mean of the latent distribution.\n",
    "            logvar (Tensor): Log-variance of the latent distribution.\n",
    "\n",
    "        Returns:\n",
    "            z (Tensor): Latent variable sampled from N(mu, sigma^2).\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)  # Convert log variance to standard deviation\n",
    "        eps = torch.randn_like(std)    # Sample noise from a normal distribution\n",
    "        return mu + std * eps\n",
    "\n",
    "    def calculate_flattened_size(self, model, input_shape):\n",
    "       #same as the last one \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            output = model(dummy_input)\n",
    "            return output.numel(), output.shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the VAE:\n",
    "        \n",
    "        1) Encode input into latent distribution parameters (mu, logvar).\n",
    "        2) Sample z using the reparameterization trick.\n",
    "        3) Decode z back to a reconstructed image.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input images.\n",
    "\n",
    "        Returns:\n",
    "            recon_x (Tensor): Reconstructed images.\n",
    "            mu (Tensor): Mean of latent distribution.\n",
    "            logvar (Tensor): Log-variance of latent distribution.\n",
    "            z (Tensor): Sampled latent variable.\n",
    "        \"\"\"\n",
    "        # Encode input\n",
    "        x_encoded = self.encoder(x)\n",
    "\n",
    "        # Compute mu and logvar\n",
    "        mu = self.fc_mu(x_encoded)\n",
    "        logvar = self.fc_logvar(x_encoded)\n",
    "\n",
    "        # Sample from the latent distribution\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decode latent vector to reconstruct the input\n",
    "        recon_x = self.decoder(z)\n",
    "        \n",
    "        return recon_x, mu, logvar, z\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
